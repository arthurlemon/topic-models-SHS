{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation des modèles LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OBJECTIF GENERAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluer les résultats des modèles LDA à l'aide de mesures automatiques**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OBJECTIFS SPECIFIQUES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculer **les mesures de perplexité et de cohérence** pour les modèles entraînés sur les 3 revues\n",
    "- Comparer **l'influence du nombre de thèmes, de la lemmatisation et de la représentation de mots sur la perplexité** des modèles\n",
    "- Etudier la **fiabilité** du modèle à l'aide de la **variabilité des mesures de perplexité, de cohérence et de similarité sémantique**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import des bibliothèques et données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim import models\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports AE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste de jetons pour chaque article\n",
    "%store -r tokens_bigrams_Corpus_LDA_AE_clean \n",
    "%store -r tokens_bigrams_Corpus_LDA_AE_clean_lemma \n",
    "\n",
    "# Set d'entraînement/test et dictionnaire\n",
    "%store -r train_texts_AE\n",
    "%store -r test_texts_AE\n",
    "%store -r dictionary_AE_2\n",
    "%store -r dictionary_AE_2_lemma\n",
    "\n",
    "# Matrice documents/jetons\n",
    "\n",
    "# Sac de mots sans lemmatisation\n",
    "%store -r corpus_train_AE\n",
    "%store -r corpus_test_AE\n",
    "\n",
    "# Sac de mots avec lemmatisation\n",
    "%store -r corpus_train_AE_lemma\n",
    "%store -r corpus_test_AE_lemma\n",
    "\n",
    "# Tf-idf sans lemmatisation\n",
    "%store -r corpus_train_AE_tfidf\n",
    "%store -r corpus_test_AE_tfidf\n",
    "\n",
    "# Tf-idf avec lemmatisation\n",
    "%store -r corpus_train_AE_tfidf_lemma\n",
    "%store -r corpus_test_AE_tfidf_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports EI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste de tokens pour chaque article\n",
    "%store -r tokens_bigrams_Corpus_LDA_EI_clean \n",
    "%store -r tokens_bigrams_Corpus_LDA_EI_clean_lemma \n",
    "\n",
    "# Set d'entraînement/test et dictionnaire\n",
    "%store -r train_texts_EI\n",
    "%store -r test_texts_EI\n",
    "%store -r dictionary_EI_2\n",
    "%store -r dictionary_EI_2_lemma\n",
    "\n",
    "# Matrice documents/jetons\n",
    "\n",
    "# Sac de mots sans lemmatisation\n",
    "%store -r corpus_train_EI\n",
    "%store -r corpus_test_EI\n",
    "\n",
    "# Sac de mots avec lemmatisation\n",
    "%store -r corpus_train_EI_lemma\n",
    "%store -r corpus_test_EI_lemma\n",
    "\n",
    "# Tf-idf sans lemmatisation\n",
    "%store -r corpus_train_EI_tfidf\n",
    "%store -r corpus_test_EI_tfidf\n",
    "\n",
    "# Tf-idf avec lemmatisation\n",
    "%store -r corpus_train_EI_tfidf_lemma\n",
    "%store -r corpus_test_EI_tfidf_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports RI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste de tokens pour chaque article\n",
    "%store -r tokens_bigrams_Corpus_LDA_RI_clean \n",
    "%store -r tokens_bigrams_Corpus_LDA_RI_clean_lemma \n",
    "\n",
    "# Set d'entraînement/test et dictionnaire\n",
    "%store -r train_texts_RI\n",
    "%store -r test_texts_RI\n",
    "%store -r dictionary_RI_2\n",
    "%store -r dictionary_RI_2_lemma\n",
    "\n",
    "# Matrice documents/jetons\n",
    "\n",
    "# Sac de mots sans lemmatisation\n",
    "%store -r corpus_train_RI\n",
    "%store -r corpus_test_RI\n",
    "\n",
    "# Sac de mots avec lemmatisation\n",
    "%store -r corpus_train_RI_lemma\n",
    "%store -r corpus_test_RI_lemma\n",
    "\n",
    "# Tf-idf sans lemmatisation\n",
    "%store -r corpus_train_RI_tfidf\n",
    "%store -r corpus_test_RI_tfidf\n",
    "\n",
    "# Tf-idf avec lemmatisation\n",
    "%store -r corpus_train_RI_tfidf_lemma\n",
    "%store -r corpus_test_RI_tfidf_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. MESURE DE PERPLEXITE POUR CHAQUE REVUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = [x for x in range(1,10)] + [x for x in range(10,110,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perplexités (num_topics, perplexité, perplexité_lemma, perplexité_tfidf, perplexité_tfidf_lemma, revue ='AE'):\n",
    "    perplexité = [element[1] for element in perplexité]\n",
    "    perplexité_lemma = [element[1] for element in perplexité_lemma]\n",
    "    perplexité_tfidf = [element[1] for element in perplexité_tfidf]\n",
    "    perplexité_tfidf_lemma = [element[1] for element in perplexité_tfidf_lemma]\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,8))\n",
    "    plt.plot(num_topics, perplexité, 'kx', label = 'sac-de-mot',markersize=10)\n",
    "    plt.plot(num_topics, perplexité_tfidf_lemma, 'yx', label = 'sac-de-mot_lemma',markersize=10)\n",
    "    plt.plot(num_topics, perplexité_tfidf, 'ko', label = 'tfidf',markersize=10)\n",
    "    plt.plot(num_topics, perplexité_tfidf_lemma, 'yo', label = 'tfidf_lemma',markersize=10)\n",
    "    plt.legend()\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.xlabel('Nombre de thèmes',fontsize=20)\n",
    "    plt.ylabel('Perplexité du modèle',fontsize=20)\n",
    "    plt.title(revue,fontsize=40)\n",
    "    plt.savefig(\"Plots/LDA/perplexité/perplexités_\" + revue + \".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVUE AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODELE SAC DE MOTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer si des valeurs ont déjà été calculées sans lemmatisation\n",
    "%store -r perplexité_AE\n",
    "# Récupérer si des valeurs ont déjà été calculées avec lemmatisation\n",
    "%store -r perplexité_AE_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perplexité_AE = []\n",
    "for num_topic in tqdm(num_topics):\n",
    "    model = LdaModel.load('Résultats_LDA/AE/lda_ae_'+ str(num_topic))\n",
    "    perplexité_AE.append((num_topic,model.log_perplexity(corpus_test_AE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perplexité_AE_lemma = []\n",
    "for num_topic in tqdm(num_topics):\n",
    "    model = LdaModel.load('Résultats_LDA/AE/lda_ae_lemma_'+ str(num_topic))\n",
    "    perplexité_AE_lemma.append((num_topic,model.log_perplexity(corpus_test_AE_lemma)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODELE TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer si des valeurs ont déjà été calculées sans lemmatisation\n",
    "%store -r perplexité_AE_tfidf\n",
    "# Récupérer si des valeurs ont déjà été calculées avec lemmatisation\n",
    "%store -r perplexité_AE_tfidf_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sans lemmatisation\n",
    "#perplexité_AE_tfidf = [] \n",
    "for num_topic in tqdm(num_topics):\n",
    "    model = LdaModel.load('Résultats_LDA/AE/lda_ae_tfidf'+ str(num_topic))\n",
    "    perplexité_AE_tfidf.append((num_topic,model.log_perplexity(corpus_test_AE_tfidf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avec lemmatisation\n",
    "#perplexité_AE_tfidf_lemma = []\n",
    "for num_topic in tqdm(num_topics):\n",
    "    if num_topic <10:\n",
    "        model = LdaModel.load('Résultats_LDA/EI/lda_ae_tfidf_lemma_'+ str(num_topic))\n",
    "    else:\n",
    "        model = LdaModel.load('Résultats_LDA/EI/lda_ae_tfidf_lemma'+ str(num_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perplexités (num_topics, perplexité_AE, perplexité_AE_lemma, perplexité_AE_tfidf, perplexité_AE_tfidf_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVUE EI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODELE SAC DE MOTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer si des valeurs ont déjà été calculées sans lemmatisation\n",
    "%store -r perplexité_EI\n",
    "# Récupérer si des valeurs ont déjà été calculées avec lemmatisation\n",
    "%store -r perplexité_EI_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perplexité_EI = []\n",
    "for num_topic in tqdm(num_topics):\n",
    "    model = LdaModel.load('Résultats_LDA/EI/lda_ei_'+ str(num_topic))\n",
    "    perplexité_EI.append((num_topic,model.log_perplexity(corpus_test_EI)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perplexité_EI_lemma = []\n",
    "for num_topic in tqdm(num_topics):\n",
    "    model = LdaModel.load('Résultats_LDA/EI/lda_ei_lemma_'+ str(num_topic))\n",
    "    perplexité_EI_lemma.append((num_topic,model.log_perplexity(corpus_test_EI_lemma)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODELE TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer si des valeurs ont déjà été calculées sans lemmatisation\n",
    "%store -r perplexité_EI_tfidf\n",
    "# Récupérer si des valeurs ont déjà été calculées avec lemmatisation\n",
    "%store -r perplexité_EI_tfidf_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sans lemmatisation\n",
    "#perplexité_EI_tfidf = [] \n",
    "for num_topic in tqdm(num_topics):\n",
    "    model = LdaModel.load('Résultats_LDA/EI/lda_ei_tfidf'+ str(num_topic))\n",
    "    perplexité_EI_tfidf.append((num_topic,model.log_perplexity(corpus_test_EI_tfidf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avec lemmatisation\n",
    "#perplexité_EI_tfidf_lemma = []\n",
    "for num_topic in tqdm(num_topics):\n",
    "    if num_topic <10:\n",
    "        model = LdaModel.load('Résultats_LDA/EI/lda_ei_tfidf_lemma_'+ str(num_topic))\n",
    "    else:\n",
    "        model = LdaModel.load('Résultats_LDA/EI/lda_ei_tfidf_lemma'+ str(num_topic))\n",
    "    perplexité_EI_tfidf_lemma.append((num_topic,model.log_perplexity(corpus_test_EI_tfidf_lemma)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perplexités (num_topics, perplexité_EI, perplexité_EI_lemma, perplexité_EI_tfidf, perplexité_EI_tfidf_lemma,revue='EI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVUE RI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODELE SAC DE MOTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer si des valeurs ont déjà été calculées sans lemmatisation\n",
    "%store -r perplexité_RI\n",
    "# Récupérer si des valeurs ont déjà été calculées avec lemmatisation\n",
    "%store -r perplexité_RI_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perplexité_RI = []\n",
    "for num_topic in tqdm(num_topics):\n",
    "    model = LdaModel.load('Résultats_LDA/RI/lda_ri_'+ str(num_topic))\n",
    "    perplexité_RI.append((num_topic,model.log_perplexity(corpus_test_RI)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perplexité_RI_lemma = []\n",
    "for num_topic in tqdm(num_topics):\n",
    "    model = LdaModel.load('Résultats_LDA/RI/lda_ri_lemma_'+ str(num_topic))\n",
    "    perplexité_RI_lemma.append((num_topic,model.log_perplexity(corpus_test_RI_lemma)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODELE TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer si des valeurs ont déjà été calculées sans lemmatisation\n",
    "%store -r perplexité_RI_tfidf\n",
    "# Récupérer si des valeurs ont déjà été calculées avec lemmatisation\n",
    "%store -r perplexité_RI_tfidf_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sans lemmatisation\n",
    "#perplexité_RI_tfidf = [] \n",
    "for num_topic in tqdm(num_topics):\n",
    "    model = LdaModel.load('Résultats_LDA/RI/lda_ri_tfidf'+ str(num_topic))\n",
    "    perplexité_RI_tfidf.append((num_topic,model.log_perplexity(corpus_test_RI_tfidf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avec lemmatisation\n",
    "#perplexité_RI_tfidf_lemma = []\n",
    "for num_topic in tqdm(num_topics):\n",
    "    model = LdaModel.load('Résultats_LDA/RI/lda_ri_tfidf_lemma'+ str(num_topic))\n",
    "    perplexité_RI_tfidf_lemma.append((num_topic,model.log_perplexity(corpus_test_RI_tfidf_lemma)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perplexités (num_topics, perplexité_RI, perplexité_RI_lemma, perplexité_RI_tfidf, perplexité_RI_tfidf_lemma, revue='RI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. COHERENCE DES MODELES POUR CHAQUE REVUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparaison des 4 mesures de convergence : (cf. Röder et al. 2015)\n",
    "- UCI coherence : utilise mesure de co-occurence basée sur des articles de Wikipedia (mesure extrinsèque)\n",
    "- Umass coherence : intrinsèque.\n",
    "- NPMI : utilise le vecteur de contexte autour de chaque \"topic top word\"\n",
    "- CV : combinaison de NPMI et fenêtre glissante de contexte : meilleure corrélation avec humain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque** : seule la configuration \"sac de mots sans lemmatisation\" a été évaluée. \n",
    "\n",
    "Pour évaluer les modèles avec lemmatisation, il faut prendre le dictionnaire lemmatisé.\n",
    "\n",
    "Puis il faut adapter les corpus utilisés en fonction de la configuration tfidf/lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running time moyen pour chaque mesure sur les 10 modèles de chaque revue = \n",
    "- 15s  pour mesure de cohérence umass\n",
    "- 15min pour mesure de cohérence c_uci\n",
    "- 15 min pour mesure de cohérence c_npmi\n",
    "- 1h pour mesure de cohérence c_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coherence (dico_coherences_AE, dico_coherences_EI, dico_coherences_RI, label = 'umass', size=(10,5)):\n",
    "    coherences_ae, coherences_ei,coherences_ri = dico_coherences_AE[label],dico_coherences_EI[label], dico_coherences_RI[label]\n",
    "    x, y_ae = zip(*coherences_ae) \n",
    "    _, y_ei = zip(*coherences_ei) \n",
    "    _, y_ri = zip(*coherences_ri) \n",
    "    \n",
    "    fig = plt.figure(figsize=size)\n",
    "    plt.plot(x,y_ae,'gx', label ='AE')\n",
    "    plt.plot(x,y_ei,'yx', label ='EI')\n",
    "    plt.plot(x,y_ri,'ox', label ='RI')\n",
    "    plt.legend()\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.xlabel('Nombre de thèmes',fontsize=20)\n",
    "    plt.ylabel('Cohérence du modèle',fontsize=20)\n",
    "    plt.savefig(\"Plots/LDA/cohérence/Comparatif/cohérences_\" + label + \".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVUE AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAC DE MOTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r dico_coherences_AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dico_coherences_AE = {'umass':[], 'c_v': [], 'uci':[], 'npmi' : []}\n",
    "dictionary = dictionary_AE_2\n",
    "temp = dictionary[0] # load du dictionnaire\n",
    "for num_topic in tqdm(range(1,10)):\n",
    "    model = LdaModel.load('Résultats_LDA/AE/lda_ae_'+ str(num_topic))\n",
    "    #dico_coherences_AE['umass'].append((num_topic,CoherenceModel(model=model,corpus=corpus_train_AE, coherence=\"u_mass\").get_coherence()))\n",
    "    dico_coherences_AE['c_v'].append((num_topic,CoherenceModel(model = model,dictionary=dictionary, coherence=\"c_v\", texts=train_texts_AE).get_coherence()))\n",
    "    dico_coherences_AE['uci'].append((num_topic,CoherenceModel(model = model,dictionary=dictionary, coherence=\"c_uci\", texts=train_texts_AE).get_coherence()))\n",
    "    dico_coherences_AE['npmi'].append((num_topic,CoherenceModel(model = model,dictionary=dictionary, coherence=\"c_npmi\", texts=train_texts_AE).get_coherence()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVUE EI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAC DE MOTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r dico_coherences_EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dico_coherences_EI = {'umass':[], 'c_v': [], 'uci':[], 'npmi' : []}\n",
    "\n",
    "dictionary = dictionary_EI_2\n",
    "temp = dictionary[0] # load du dictionnaire\n",
    "\n",
    "for num_topic in tqdm(range(1,10)):\n",
    "    model = LdaModel.load('Résultats_LDA/EI/lda_ei_'+ str(num_topic))\n",
    "    #dico_coherences_EI['umass'].append((num_topic,CoherenceModel(model=model,corpus=corpus_train_EI, coherence=\"u_mass\").get_coherence()))\n",
    "    dico_coherences_EI['c_v'].append((num_topic,CoherenceModel(model = model,dictionary=dictionary, coherence=\"c_v\", texts=train_texts_EI).get_coherence()))\n",
    "    #dico_coherences_EI['uci'].append((num_topic,CoherenceModel(model = model,dictionary=dictionary, coherence=\"c_uci\", texts=train_texts_EI).get_coherence()))\n",
    "    #dico_coherences_EI['npmi'].append((num_topic,CoherenceModel(model = model,dictionary=dictionary, coherence=\"c_npmi\", texts=train_texts_EI).get_coherence()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVUE RI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAC DE MOTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r dico_coherences_RI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dico_coherences_RI = {'umass':[], 'c_v': [], 'uci':[], 'npmi' : []}\n",
    "\n",
    "dictionary = dictionary_RI_2\n",
    "temp = dictionary[0] # load du dictionnaire\n",
    "\n",
    "for num_topic in tqdm(range(1,10)):\n",
    "    model = LdaModel.load('Résultats_LDA/RI/lda_ri_'+ str(num_topic))\n",
    "    dico_coherences_RI['umass'].append((num_topics[num_topic],CoherenceModel(model=model,corpus=corpus_train_RI, coherence=\"u_mass\").get_coherence()))\n",
    "    dico_coherences_RI['c_v'].append((num_topics[num_topic],CoherenceModel(model = model,dictionary=dictionary, coherence=\"c_v\", texts=train_texts_RI).get_coherence()))\n",
    "    dico_coherences_RI['uci'].append((num_topic,CoherenceModel(model = model,dictionary=dictionary, coherence=\"c_uci\", texts=train_texts_RI).get_coherence()))\n",
    "    dico_coherences_RI['npmi'].append((num_topics[num_topic],CoherenceModel(model = model,dictionary=dictionary, coherence=\"c_npmi\", texts=train_texts_RI).get_coherence()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOTS POUR LES 3 REVUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coherence (dico_coherences_AE, dico_coherences_EI, dico_coherences_RI, label = 'umass', size=(10,5)):\n",
    "    coherences_ae, coherences_ei,coherences_ri = dico_coherences_AE[label],dico_coherences_EI[label], dico_coherences_RI[label]\n",
    "    x, y_ae = zip(*coherences_ae) \n",
    "    _, y_ei = zip(*coherences_ei) \n",
    "    _, y_ri = zip(*coherences_ri) \n",
    "    \n",
    "    fig = plt.figure(figsize=size)\n",
    "    plt.plot(x,y_ae,'gx', label ='AE')\n",
    "    plt.plot(x,y_ei,'yo', label ='EI')\n",
    "    plt.plot(x,y_ri,'rs', label ='RI')\n",
    "    plt.legend()\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.title(label,fontsize=20)\n",
    "    plt.xlabel('Nombre de thèmes',fontsize=20)\n",
    "    plt.ylabel('Cohérence du modèle',fontsize=20)\n",
    "    plt.savefig(\"Plots/LDA/cohérence/Comparatif/cohérences_\" + label + \".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r dico_coherences_AE\n",
    "%store -r dico_coherences_EI\n",
    "%store -r dico_coherences_RI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence (dico_coherences_AE, dico_coherences_EI, dico_coherences_RI, label = 'umass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence (dico_coherences_AE, dico_coherences_EI, dico_coherences_RI, label = 'c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence (dico_coherences_AE, dico_coherences_EI, dico_coherences_RI, label = 'uci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence (dico_coherences_AE, dico_coherences_EI, dico_coherences_RI, label = 'npmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. ETUDE DE FIABILITE POUR CHAQUE REVUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ETUDE DE LA PERPLEXITE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVUE AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer si des valeurs ont déjà été calculées sans lemmatisation\n",
    "%store -r perplexité_AE_10\n",
    "%store -r perplexité_AE_40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiabilité LDA 10 thèmes\n",
    "#perplexité_AE_10 = []\n",
    "indexes = [0,1,2,3,4]\n",
    "for index in tqdm(indexes):\n",
    "    model = LdaModel.load('Résultats_LDA/Fiabilité/lda_ae_10_'+ str(index))\n",
    "    perplexité_AE_10.append((index,model.log_perplexity(corpus_test_AE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiabilité LDA 40 thèmes\n",
    "#perplexité_AE_40 = []\n",
    "indexes = [0,1,2,3,4]\n",
    "for index in tqdm(indexes):\n",
    "    model = LdaModel.load('Résultats_LDA/Fiabilité/lda_ae_40_'+ str(index))\n",
    "    perplexité_AE_40.append((index,model.log_perplexity(corpus_test_AE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVUE EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer si des valeurs ont déjà été calculées sans lemmatisation\n",
    "%store -r perplexité_EI_10\n",
    "%store -r perplexité_EI_40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiabilité LDA 10 thèmes\n",
    "#perplexité_EI_10 = []\n",
    "indexes = [0,1,2,3,4]\n",
    "for index in tqdm(indexes):\n",
    "    model = LdaModel.load('Résultats_LDA/Fiabilité/lda_ei_10_'+ str(index))\n",
    "    perplexité_EI_10.append((index,model.log_perplexity(corpus_test_EI)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiabilité LDA 40 thèmes\n",
    "#perplexité_EI_40 = []\n",
    "indexes = [0,1,2,3,4]\n",
    "for index in tqdm(indexes):\n",
    "    model = LdaModel.load('Résultats_LDA/Fiabilité/lda_ei_40_'+ str(index))\n",
    "    perplexité_EI_40.append((index,model.log_perplexity(corpus_test_EI)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVUE RI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer si des valeurs ont déjà été calculées sans lemmatisation\n",
    "%store -r perplexité_RI_10\n",
    "%store -r perplexité_RI_40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiabilité LDA 10 thèmes\n",
    "#perplexité_RI_10 = []\n",
    "indexes = [0,1,2,3,4]\n",
    "for index in tqdm(indexes):\n",
    "    model = LdaModel.load('Résultats_LDA/Fiabilité/lda_ri_10_'+ str(index))\n",
    "    perplexité_RI_10.append((index,model.log_perplexity(corpus_test_RI)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiabilité LDA 10 thèmes\n",
    "#perplexité_RI_40 = []\n",
    "indexes = [0,1,2,3,4]\n",
    "for index in tqdm(indexes):\n",
    "    model = LdaModel.load('Résultats_LDA/Fiabilité/lda_ri_40_'+ str(index))\n",
    "    perplexité_RI_40.append((index,model.log_perplexity(corpus_test_RI)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOTS POUR LES 3 REVUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r perplexité_AE_10\n",
    "%store -r perplexité_AE_40\n",
    "\n",
    "%store -r perplexité_EI_10\n",
    "%store -r perplexité_EI_40\n",
    "\n",
    "%store -r perplexité_RI_10\n",
    "%store -r perplexité_RI_40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_10 = [element[1] for element in perplexité_AE_10]\n",
    "AE_40 = [element[1] for element in perplexité_AE_40]\n",
    "EI_10 = [element[1] for element in perplexité_EI_10]\n",
    "EI_40 = [element[1] for element in perplexité_EI_40]\n",
    "RI_10 = [element[1] for element in perplexité_RI_10]\n",
    "RI_40 = [element[1] for element in perplexité_RI_40]\n",
    "\n",
    "numpy_AE_10 = np.array(AE_10)\n",
    "numpy_AE_40 = np.array(AE_40)\n",
    "numpy_EI_10 = np.array(EI_10)\n",
    "numpy_EI_40 = np.array(EI_40)\n",
    "numpy_RI_10 = np.array(RI_10)\n",
    "numpy_RI_40 = np.array(RI_40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "BoxName = ['LDA AE 10', 'LDA EI 10', 'LDA RI 10','LDA AE 40', 'LDA EI 40', 'LDA RI 40']\n",
    "data = [AE_10,EI_10,RI_10,AE_40,EI_40,RI_40]\n",
    "plt.boxplot(data)\n",
    "plt.ylim(-11,-9.5)\n",
    "plt.xticks([1,2,3,4,5,6], BoxName)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel('Perplexité',fontsize=20)\n",
    "plt.savefig(\"Plots/LDA/perplexité/perplexités_fiabilités_3_REVUES.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Perplexité des modèles à 10 thèmes pour AE :\", round(np.mean(numpy_AE_10),3), \"+/-\", round(np.std(numpy_AE_10),3))\n",
    "print(\"Perplexité des modèles à 40 thèmes pour AE :\", round(np.mean(numpy_AE_40),3), \"+/-\", round(np.std(numpy_AE_40),3))\n",
    "print(\"Perplexité des modèles à 10 thèmes pour RI :\", round(np.mean(numpy_EI_10),3), \"+/-\", round(np.std(numpy_EI_10),3))\n",
    "print(\"Perplexité des modèles à 40 thèmes pour RI :\", round(np.mean(numpy_EI_40),3), \"+/-\", round(np.std(numpy_EI_40),3))\n",
    "print(\"Perplexité des modèles à 10 thèmes pour RI :\", round(np.mean(numpy_RI_10),3), \"+/-\", round(np.std(numpy_RI_10),3))\n",
    "print(\"Perplexité des modèles à 40 thèmes pour RI :\", round(np.mean(numpy_RI_40),3), \"+/-\", round(np.std(numpy_RI_40),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ETUDE DE LA COHERENCE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque revue, running time ~ 30min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVUE AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_coherences_AE_fiabilité = {'10' : {'umass':[], 'c_v': [], 'uci':[], 'npmi' : []},'40' : {'umass':[], 'c_v': [], 'uci':[], 'npmi' : []}}\n",
    "\n",
    "dictionary = dictionary_AE_2\n",
    "temp = dictionary[0] # load du dictionnaire\n",
    "\n",
    "indexes = [0,1,2,3,4]\n",
    "for num_topic in tqdm(['10','40']):\n",
    "    for index in tqdm(indexes):\n",
    "        #model = LdaModel.load('Résultats_LDA/Fiabilité/lda_ae_'+ num_topic+ '_'+str(index))\n",
    "        model = LdaModel.load('Résultats_LDA/lda_ae_'+ num_topic+ '_'+str(index))\n",
    "        try :\n",
    "            dico_coherences_AE_fiabilité[num_topic]['umass'].append(CoherenceModel(model=model,corpus=corpus_train_AE, coherence=\"u_mass\").get_coherence())\n",
    "        except: \n",
    "            print('Problème avec ' + 'lda_ae_'+ num_topic+ '_'+str(index)+ ' pour umass')\n",
    "        try :\n",
    "            dico_coherences_AE_fiabilité[num_topic]['c_v'].append(CoherenceModel(model = model,dictionary=dictionary, coherence=\"c_v\", texts=train_texts_AE).get_coherence())\n",
    "        except: \n",
    "            print('Problème avec ' + 'lda_ae_'+ num_topic+ '_'+str(index) + ' pour c_v')\n",
    "        try :\n",
    "            dico_coherences_AE_fiabilité[num_topic]['uci'].append(CoherenceModel(model = model,dictionary=dictionary, coherence=\"c_uci\", texts=train_texts_AE).get_coherence())\n",
    "        except:\n",
    "             print('Problème avec ' + 'lda_ae_'+ num_topic+ '_'+str(index) + ' pour uci')\n",
    "        try:\n",
    "            dico_coherences_AE_fiabilité[num_topic]['npmi'].append(CoherenceModel(model = model,dictionary=dictionary, coherence=\"c_npmi\", texts=train_texts_AE).get_coherence())\n",
    "        except :\n",
    "             print('Problème avec ' + 'lda_ae_'+ num_topic+ '_'+str(index) + 'pour npmi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store dico_coherences_AE_fiabilité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_coherences_AE_fiabilité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVUE EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_coherences_EI_fiabilité = {'10' : {'umass':[], 'c_v': [], 'uci':[], 'npmi' : []},'40' : {'umass':[], 'c_v': [], 'uci':[], 'npmi' : []}}\n",
    "\n",
    "dictionary = dictionary_EI_2\n",
    "temp = dictionary[0] # load du dictionnaire\n",
    "\n",
    "indexes = [0,1,2,3,4]\n",
    "for num_topic in tqdm(['10','40']):\n",
    "    for index in tqdm(indexes):\n",
    "        model = LdaModel.load('Résultats_LDA/Fiabilité/lda_ei_'+ num_topic+ '_'+str(index))\n",
    "        try :\n",
    "            dico_coherences_EI_fiabilité[num_topic]['umass'].append(CoherenceModel(model=model,corpus=corpus_train_EI, coherence=\"u_mass\").get_coherence())\n",
    "        except: \n",
    "            print('Problème avec ' + 'lda_ei_'+ num_topic+ '_'+str(index)+ ' pour c_v')\n",
    "        try :\n",
    "            dico_coherences_EI_fiabilité[num_topic]['c_v'].append(CoherenceModel(model = model,dictionary=dictionary, coherence=\"c_v\", texts=train_texts_EI).get_coherence())\n",
    "        except: \n",
    "            print('Problème avec ' + 'lda_ei_'+ num_topic+ '_'+str(index) + ' pour c_v')\n",
    "        try :\n",
    "            dico_coherences_EI_fiabilité[num_topic]['uci'].append(CoherenceModel(model = model,dictionary=dictionary, coherence=\"c_uci\", texts=train_texts_EI).get_coherence())\n",
    "        except:\n",
    "             print('Problème avec ' + 'lda_ei_'+ num_topic+ '_'+str(index) + ' pour uci')\n",
    "        try:\n",
    "            dico_coherences_EI_fiabilité[num_topic]['npmi'].append(CoherenceModel(model = model,dictionary=dictionary, coherence=\"c_npmi\", texts=train_texts_EI).get_coherence())\n",
    "        except :\n",
    "             print('Problème avec ' + 'lda_ei_'+ num_topic+ '_'+str(index) + 'pour npmi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_coherences_EI_fiabilité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVUE RI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_coherences_RI_fiabilité = {'10' : {'umass':[], 'c_v': [], 'uci':[], 'npmi' : []},'40' : {'umass':[], 'c_v': [], 'uci':[], 'npmi' : []}}\n",
    "\n",
    "dictionary = dictionary_RI_2\n",
    "temp = dictionary[0] # load du dictionnaire\n",
    "\n",
    "indexes = [0,1,2,3,4]\n",
    "for num_topic in tqdm(['10','40']):\n",
    "    for index in tqdm(indexes):\n",
    "        model = LdaModel.load('Résultats_LDA/Fiabilité/lda_ri_'+ num_topic+ '_'+str(index))\n",
    "        try :\n",
    "            dico_coherences_RI_fiabilité[num_topic]['umass'].append(CoherenceModel(model=model,corpus=corpus_train_RI, coherence=\"u_mass\").get_coherence())\n",
    "        except: \n",
    "            print('Problème avec ' + 'lda_ri_'+ num_topic+ '_'+str(index)+ ' pour c_v')\n",
    "        try :\n",
    "            dico_coherences_RI_fiabilité[num_topic]['c_v'].append(CoherenceModel(model = model,dictionary=dictionary, coherence=\"c_v\", texts=train_texts_RI).get_coherence())\n",
    "        except: \n",
    "            print('Problème avec ' + 'lda_ri_'+ num_topic+ '_'+str(index) + ' pour c_v')\n",
    "        try :\n",
    "            dico_coherences_RI_fiabilité[num_topic]['uci'].append(CoherenceModel(model = model,dictionary=dictionary, coherence=\"c_uci\", texts=train_texts_RI).get_coherence())\n",
    "        except:\n",
    "             print('Problème avec ' + 'lda_ri_'+ num_topic+ '_'+str(index) + ' pour uci')\n",
    "        try:\n",
    "            dico_coherences_RI_fiabilité[num_topic]['npmi'].append(CoherenceModel(model = model,dictionary=dictionary, coherence=\"c_npmi\", texts=train_texts_RI).get_coherence())\n",
    "        except :\n",
    "             print('Problème avec ' + 'lda_ri_'+ num_topic+ '_'+str(index) + 'pour npmi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_coherences_RI_fiabilité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOTS POUR LES 3 REVUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r dico_coherences_AE_fiabilité\n",
    "%store -r dico_coherences_EI_fiabilité\n",
    "%store -r dico_coherences_RI_fiabilité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variabilité(label='umass'):\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    BoxName = ['LDA AE 10', 'LDA EI 10', 'LDA RI 10','LDA AE 40', 'LDA EI 40', 'LDA RI 40']\n",
    "    AE_10 = dico_coherences_AE_fiabilité['10'][label]\n",
    "    AE_40 = dico_coherences_AE_fiabilité['40'][label]\n",
    "    EI_10 = dico_coherences_EI_fiabilité['10'][label]\n",
    "    EI_40 = dico_coherences_EI_fiabilité['40'][label]\n",
    "    RI_10 = dico_coherences_RI_fiabilité['10'][label]\n",
    "    RI_40 = dico_coherences_RI_fiabilité['40'][label]\n",
    "    data = [AE_10,EI_10,RI_10,AE_40,EI_40,RI_40]\n",
    "    plt.boxplot(data)\n",
    "    plt.ylim(-0.5,0.5)\n",
    "    plt.xticks([1,2,3,4,5,6], BoxName, fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.title(label,fontsize=20)\n",
    "    plt.ylabel('Cohérence',fontsize=20)\n",
    "    plt.savefig(\"Plots/LDA/cohérence/comparatif/cohérence_fiabilité_\" + label + \".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variabilité(label='umass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variabilité(label='npmi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variabilité(label='c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variabilité(label='uci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_umass_10 =np.array(dico_coherences_AE_fiabilité ['10']['umass'])\n",
    "AE_uci_10 =np.array(dico_coherences_AE_fiabilité['10']['uci'])\n",
    "AE_npmi_10 = np.array(dico_coherences_AE_fiabilité['10']['npmi'])\n",
    "AE_cv_10 = np.array(dico_coherences_AE_fiabilité['10']['c_v'])\n",
    "\n",
    "AE_umass_40 = np.array(dico_coherences_AE_fiabilité['40']['umass'])\n",
    "AE_uci_40 = np.array(dico_coherences_AE_fiabilité['40']['uci'])\n",
    "AE_npmi_40 = np.array(dico_coherences_AE_fiabilité['40']['npmi'])\n",
    "AE_cv_40 = np.array(dico_coherences_AE_fiabilité['40']['c_v'])\n",
    "\n",
    "EI_umass_10 = np.array(dico_coherences_EI_fiabilité['10']['umass'])\n",
    "EI_uci_10 = np.array(dico_coherences_EI_fiabilité['10']['uci'])\n",
    "EI_npmi_10 =np.array(dico_coherences_EI_fiabilité['10']['npmi'])\n",
    "EI_cv_10 = np.array(dico_coherences_EI_fiabilité['10']['c_v'])\n",
    "\n",
    "EI_umass_40 = np.array(dico_coherences_EI_fiabilité['40']['umass'])\n",
    "EI_uci_40 = np.array(dico_coherences_EI_fiabilité['40']['uci'])\n",
    "EI_npmi_40 = np.array(dico_coherences_EI_fiabilité['40']['npmi'])\n",
    "EI_cv_40 = np.array(dico_coherences_EI_fiabilité['40']['c_v'])\n",
    "\n",
    "RI_umass_10 = np.array(dico_coherences_RI_fiabilité['10']['umass'])\n",
    "RI_uci_10 = np.array(dico_coherences_RI_fiabilité['10']['uci'])\n",
    "RI_npmi_10 = np.array(dico_coherences_RI_fiabilité['10']['npmi'])\n",
    "RI_cv_10 = np.array(dico_coherences_RI_fiabilité['10']['c_v'])\n",
    "\n",
    "RI_umass_40 = np.array(dico_coherences_RI_fiabilité['40']['umass'])\n",
    "RI_uci_40 = np.array(dico_coherences_RI_fiabilité['40']['uci'])\n",
    "RI_npmi_40 = np.array(dico_coherences_RI_fiabilité['40']['npmi'])\n",
    "RI_cv_40 = np.array(dico_coherences_RI_fiabilité['40']['c_v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cohérence des modèles à 10 thèmes pour AE:\", round(np.mean(AE_umass_10),3), \"+/-\", round(np.std(AE_umass_10),3))\n",
    "print(\"Cohérence des modèles à 40 thèmes pour AE :\", round(np.mean(AE_umass_40),3), \"+/-\", round(np.std(AE_umass_40),3))\n",
    "print(\"Cohérence des modèles à 10 thèmes pour AE :\", round(np.mean(AE_uci_10),3), \"+/-\", round(np.std(AE_uci_10),3))\n",
    "print(\"Cohérence des modèles à 40 thèmes pour AE :\", round(np.mean(AE_uci_40),3), \"+/-\", round(np.std(AE_uci_40),3))\n",
    "print(\"Cohérence des modèles à 10 thèmes pour AE :\", round(np.mean(AE_npmi_10),3), \"+/-\", round(np.std(AE_npmi_10),3))\n",
    "print(\"Cohérence des modèles à 40 thèmes pour AE :\", round(np.mean(AE_npmi_40),3), \"+/-\", round(np.std(AE_npmi_40),3))\n",
    "print(\"Cohérence des modèles à 10 thèmes pour AE :\", round(np.mean(AE_cv_10),3), \"+/-\", round(np.std(AE_cv_10),3))\n",
    "print(\"Cohérence des modèles à 40 thèmes pour AE :\", round(np.mean(AE_cv_40),3), \"+/-\", round(np.std(AE_cv_40),3))\n",
    "\n",
    "print(\"Cohérence des modèles à 10 thèmes pour EI :\", round(np.mean(EI_umass_10),3), \"+/-\", round(np.std(EI_umass_10),3))\n",
    "print(\"Cohérence des modèles à 40 thèmes pour EI :\", round(np.mean(EI_umass_40),3), \"+/-\", round(np.std(EI_umass_40),3))\n",
    "print(\"Cohérence des modèles à 10 thèmes pour EI :\", round(np.mean(EI_uci_10),3), \"+/-\", round(np.std(EI_uci_10),3))\n",
    "print(\"Cohérence des modèles à 40 thèmes pour EI :\", round(np.mean(EI_uci_40),3), \"+/-\", round(np.std(EI_uci_40),3))\n",
    "print(\"Cohérence des modèles à 10 thèmes pour EI :\", round(np.mean(EI_npmi_10),3), \"+/-\", round(np.std(EI_npmi_10),3))\n",
    "print(\"Cohérence des modèles à 40 thèmes pour EI :\", round(np.mean(EI_npmi_40),3), \"+/-\", round(np.std(EI_npmi_40),3))\n",
    "print(\"Cohérence des modèles à 10 thèmes pour EI :\", round(np.mean(EI_cv_10),3), \"+/-\", round(np.std(EI_cv_10),3))\n",
    "print(\"Cohérence des modèles à 40 thèmes pour EI :\", round(np.mean(EI_cv_40),3), \"+/-\", round(np.std(EI_cv_40),3))\n",
    "\n",
    "print(\"Cohérence des modèles à 10 thèmes pour RI :\", round(np.mean(RI_umass_10),3), \"+/-\", round(np.std(RI_umass_10),3))\n",
    "print(\"Cohérence des modèles à 40 thèmes pour RI :\", round(np.mean(RI_umass_40),3), \"+/-\", round(np.std(RI_umass_40),3))\n",
    "print(\"Cohérence des modèles à 10 thèmes pour RI :\", round(np.mean(RI_uci_10),3), \"+/-\", round(np.std(RI_uci_10),3))\n",
    "print(\"Cohérence des modèles à 40 thèmes pour RI :\", round(np.mean(RI_uci_40),3), \"+/-\", round(np.std(RI_uci_40),3))\n",
    "print(\"Cohérence des modèles à 10 thèmes pour RI :\", round(np.mean(RI_npmi_10),3), \"+/-\", round(np.std(RI_npmi_10),3))\n",
    "print(\"Cohérence des modèles à 40 thèmes pour RI :\", round(np.mean(RI_npmi_40),3), \"+/-\", round(np.std(RI_npmi_40),3))\n",
    "print(\"Cohérence des modèles à 10 thèmes pour RI :\", round(np.mean(RI_cv_10),3), \"+/-\", round(np.std(RI_cv_10),3))\n",
    "print(\"Cohérence des modèles à 40 thèmes pour RI :\", round(np.mean(RI_cv_40),3), \"+/-\", round(np.std(RI_cv_40),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ETUDE DE LA SIMILARITE SEMANTIQUE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compte_similarité (revue ='ae'):\n",
    "    dico_similarité = {'10':[],'40':[]}\n",
    "    compte_similarité = {'10':[],'40':[]}\n",
    "    for num_topic in ['10','40']:\n",
    "        # Récupérer l'ensemble des jetons produits par chaque modèle\n",
    "        for num_model in range(5):\n",
    "            model = LdaModel.load('Résultats_LDA/Fiabilité/lda_' + revue + '_' + num_topic +'_' + str(num_model))\n",
    "            jetons = []\n",
    "            for topic in range(model.num_topics):\n",
    "                topics = model.show_topic(topic)\n",
    "                jetons.extend([element[0] for element in topics])\n",
    "            dico_similarité[num_topic].append(jetons)\n",
    "        # Compter les similarité de jetons 2 à 2 + prendre la moyenne puis pourcentage sur le total\n",
    "        for i in range (5):\n",
    "            for j in range(i,5):\n",
    "                compte_similarité[num_topic].append(len(set(dico_similarité[num_topic][i]) & set(dico_similarité[num_topic][j])))\n",
    "        compte_similarité[num_topic] = [element/int(num_topic)*10 for element in compte_similarité[num_topic]]\n",
    "        #compte_similarité[num_topic] = round(np.mean(compte_similarité[num_topic]),1)\n",
    "    return dico_similarité, compte_similarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_similarité_ae, compte_similarité_ae = compte_similarité(revue='ae')\n",
    "dico_similarité_ei, compte_similarité_ei = compte_similarité(revue='ei')\n",
    "dico_similarité_ri, compte_similarité_ri = compte_similarité(revue='ri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_resultats_stats (compte_similarité):\n",
    "    for topic in ['10','40']:\n",
    "        print(\"Similarité globale pour modèles avec \", topic, \"thèmes : \", \n",
    "              round(np.mean(compte_similarité[topic]),1), \" +/-\", round(np.std(compte_similarité[topic]),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_resultats_stats(compte_similarité_ae)\n",
    "print_resultats_stats(compte_similarité_ei)\n",
    "print_resultats_stats(compte_similarité_ri)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
