{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction des articles du serveur Erudit et stockage en mémoire locale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OBJECTIF GENERAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etablir un code propre et optimisé pour extraire le contenu et métadonnées des articles XML du serveur Erudit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OBJECTIFS SPECIFIQUES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Indexer les journaux d'intérêt dont on souhaite récupérer les articles\n",
    "- Former un corpus associant le texte et les métadonnées pour chaque article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **METHODE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I. Indexation des revues, journaux et articles d'intérêt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Création de listes contenant les indexes des journaux, revues et articles d'intérêt\n",
    "\n",
    "2 - Création de dossiers de stockage en local et stockage des objets requests (.xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**II. Formation du Corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Récupération du contenu XML des articles sous forme d'objet soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Création d'un tuple pour chaque article à partir de l'objet soup: (texte, {métadonnées : revue, auteur, date})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Aggrégation des tuples sous la forme d'une liste représentant l'ensemble d'une revue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PIPELINE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INPUT** : \n",
    "\n",
    "Fichiers XML du serveur Erudit\n",
    "\n",
    "**OUTPUT** : \n",
    "- Listes d'indexation : ref_journal - ref_revue - ref_article\n",
    "\n",
    "- Contenu articles et métadonnées : Liste de tuples de la forme (texte de l'article, {ditionnaire de métadonnées})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pickle\n",
    "import spacy\n",
    "import textacy\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. INDEXATION (à faire une seule fois)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTPUT DE CETTE PARTIE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listes pour stocker les chemins des différents articles\n",
    "% store -r ref_journal\n",
    "% store -r ref_revue\n",
    "% store -r ref_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sélection des revues d'intérêt\n",
    "\n",
    "Critère de sélection: \n",
    "- **Thématiques**: Economie, innovation, risque, management, finance, commerce international (lien avec intérêts de recherche de Catherine)\n",
    "- **Langue** : Français\n",
    "- **Periode de publication** : au moins 40 ans (pour les modèles dynamiques)\n",
    "\n",
    "Revues =  Actualité Economique, Relations Industrielles et Etudes Internationales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB : La liste ref_journal peut être modifiée en fonction des revues d'intérêt. Il suffit d'ajouter le chemin correspondant vers le serveur Erudit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REF_JOURNAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stockage du chemin de chaque revue vers le serveur Erudit\n",
    "ref_journal = ['/erudit/ae49', '/erudit/ri17', '/erudit/ei50']\n",
    "% store ref_journal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REF_REVUE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stockage des id des revues pour chaque journal\n",
    "ref_revue = [] # liste de liste où chaque élément est du type [index_revue,index_numero] = numero de publication\n",
    "for index_journal in range(len(ref_journal)):\n",
    "    h_ref = []\n",
    "    path = 'https://fichiers.erudit.org/cyber/'+ ref_journal[index_journal]\n",
    "    r = requests.get(path, auth=('mgagnon', 'iathe6eiyaiy6Eic'))\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    all_links = soup.find_all(\"a\")\n",
    "    for link in all_links:\n",
    "        h_ref.append(link.get(\"href\"))\n",
    "    ref_revue.append(h_ref)\n",
    "\n",
    "# on enlève les balises inutiles au début des documents HTML\n",
    "for j in range (5):\n",
    "    for i in range(len(ref_journal)):\n",
    "        ref_revue[i].pop(0)\n",
    "% store ref_revue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REF_ARTICLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stockage des id des articles pour chaque revue\n",
    "total_folders = []\n",
    "for i in range(len(ref_revue)):\n",
    "    total_folders.append(len(ref_revue[i]))\n",
    "\n",
    "ref_article = [] #liste de 3 colonnes avec [Revue][Numero de Publication][Article] = numéro article\n",
    "for index_journal in tqdm_notebook(range(len(ref_journal))):\n",
    "    h_ref_bis = []\n",
    "    for j in tqdm_notebook(range(total_folders[index_journal])):\n",
    "        h_ref = []\n",
    "        path = 'https://fichiers.erudit.org/cyber'+ ref_journal[index_journal] + '/' + ref_revue[index_journal][j]\n",
    "        r = requests.get(path, auth=('mgagnon', 'iathe6eiyaiy6Eic'))\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        new_links = soup.find_all(\"a\")\n",
    "        for link in new_links:\n",
    "            h_ref.append(link.get(\"href\"))  \n",
    "        h_ref_bis.append(h_ref)\n",
    "    ref_article.append(h_ref_bis)\n",
    "\n",
    "for k in range(5):\n",
    "    for index_journal in range(len(ref_journal)):\n",
    "        for j in range(total_folders[index_journal]):\n",
    "            ref_article[index_journal][j].pop(0)\n",
    "% store ref_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II.FORMATION DU CORPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTPUT DE CETTE PARTIE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération d'une liste de dictionnaire du type {'texte' : texte article brut , 'metadata': {'title' : titre, 'typeart': , 'lang': , 'annee': ,'info_auteurs': [(prenom, nom, affiliation)]}}\n",
    "%store -r Corpus_AE\n",
    "%store -r Corpus_EI\n",
    "%store -r Corpus_RI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Récupération XML en objets soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_journaux = {'AE' : 0, 'RI' : 1, 'EI' : 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USAGE:**\n",
    "\n",
    "- Premier usage : créer le dossier \"Variables résultats\"\n",
    "\n",
    "- *request_xml* doit être lancée une unique fois pour chaque revue (écriture des xml en local)\n",
    "\n",
    "- *get_soup_from_xml* doit être lancée à chaque utlisation du notebook et pour chaque revue (récupération des objets soup)\n",
    "- De même, les listes liste_AE, liste_EI, liste_RI doivent être recrées à chaque utilisation du notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_xml(ref_revue, ref_article, ref_journal, journal ='AE'):\n",
    "    \"\"\" Fonction pour stocker les requests pour tous les articles d'intérêt depuis le serveur Erudit\"\"\"\n",
    "    index_journal = index_journaux[journal]\n",
    "    for index_revue in tqdm_notebook(range(len(ref_revue[index_journal]))):\n",
    "        for index_article in range(len(ref_article[index_journal][index_revue])):\n",
    "            # Récupération du fichier XML sur le serveur et création d'un objet BeautifulSoup\n",
    "            article_path = 'https://fichiers.erudit.org/cyber' + ref_journal[index_journal] + '/' + ref_revue[index_journal][index_revue] + ref_article[index_journal][index_revue][index_article].replace(\"/\", \"\") \n",
    "            r = requests.get(article_path + '/ERUDITXSD300.xml', auth=('mgagnon', 'iathe6eiyaiy6Eic')) \n",
    "            # Ecriture en local des objets request\n",
    "            with open('Variables résultats/requests_' + journal + '/' + ref_revue[index_journal][index_revue].replace(\"/\",\"\") \n",
    "                      + '_' + ref_article[index_journal][index_revue][index_article].replace(\"/\", \"\")  +'.xml', 'w', encoding ='utf-8') as file :\n",
    "                file.write(r.text)\n",
    "                file.close()\n",
    "    print(\"Stockage des fichiers requests terminé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stockage des fichiers requests en local : à faire tourner une seule fois (=1h20 min)\n",
    "request_xml(ref_revue, ref_article, ref_journal, journal= 'AE')\n",
    "request_xml(ref_revue, ref_article, ref_journal, journal= 'RI')\n",
    "request_xml(ref_revue, ref_article, ref_journal, journal= 'EI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **USAGE** : FAIRE TOURNER LES 4 CELLULES CI-DESSOUS A CHAQUE OUVERTURE DU NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listes pour stocker les chemins des différents articles\n",
    "% store -r ref_journal\n",
    "% store -r ref_revue\n",
    "% store -r ref_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_journaux = {'AE' : 0, 'RI' : 1, 'EI' : 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup_from_xml (ref_revue, ref_article, ref_journal, journal = 'AE'):\n",
    "    soup_articles = []\n",
    "    index_journal = index_journaux[journal]\n",
    "    for index_revue in tqdm_notebook(range(len(ref_revue[index_journal]))):\n",
    "        for index_article in range(len(ref_article[index_journal][index_revue])):\n",
    "            article_path = 'https://fichiers.erudit.org/cyber' + ref_journal[index_journal] + '/' + ref_revue[index_journal][index_revue] + ref_article[index_journal][index_revue][index_article].replace(\"/\", \"\") \n",
    "            with open(\"Variables résultats/requests_\" + journal +'/' + ref_revue[index_journal][index_revue].replace(\"/\",\"\") \n",
    "                      + '_' + ref_article[index_journal][index_revue][index_article].replace(\"/\", \"\")  +'.xml', 'r', encoding ='utf-8') as r :\n",
    "                soup_articles.append((article_path, BeautifulSoup(r, \"lxml\")))\n",
    "    return soup_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer les objets soup : à refaire à chaque fois (=5 min)\n",
    "soup_articles_AE = get_soup_from_xml(ref_revue, ref_article, ref_journal, journal= 'AE')\n",
    "soup_articles_RI = get_soup_from_xml(ref_revue, ref_article, ref_journal, journal= 'RI')\n",
    "soup_articles_EI = get_soup_from_xml(ref_revue, ref_article, ref_journal, journal= 'EI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Formation du Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTPUT** : liste de tuples dont les éléments sont du type (texte, {métadonnées})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus (soup_articles):\n",
    "    \"\"\" Récupérer le texte brut et les métadonnées d'intérêt pour chaque article\"\"\"\n",
    "    Corpus = []\n",
    "    for index in tqdm_notebook(range(len(soup_articles))):\n",
    "        soup = soup_articles[index][1] # récupération de l'objet soup\n",
    "        try:\n",
    "            if soup == None: pass\n",
    "            \n",
    "            # Récupération du prénom, nom et affiliation des auteurs de l'article\n",
    "            auteurs = []\n",
    "            for auteur in soup.find_all('auteur'):\n",
    "                try: #test si l'information est complète, sinon on passe\n",
    "                    auteurs.append((auteur.prenom.text,auteur.nomfamille.text,auteur.affiliation.text))\n",
    "                except:\n",
    "                    auteurs = None\n",
    "                    break\n",
    "            # métadonnées du XML que l'on garde en mémoire    \n",
    "            metadata = {}\n",
    "            metadata['title'] =  soup.titre.text if soup.titre != None else None\n",
    "            metadata['typeart'] =  soup.article['typeart'] if soup.article != None else None\n",
    "            metadata['lang'] =  soup.article['lang'] if soup.article != None else None\n",
    "            metadata['traitement'] =  soup.article['qualtraitement'] if soup.article != None else None\n",
    "            metadata['annee'] =  soup.annee.text if soup.annee != None else None\n",
    "            metadata['info_auteurs'] =  auteurs \n",
    "\n",
    "            # Ajout du texte et des métadonnées au sein d'un tuple représentant l'article\n",
    "            Corpus.append({'texte' : soup.corps.text, 'metadata': metadata, 'URL' : soup_articles[index][0]})\n",
    "            \n",
    "        except AttributeError as error:\n",
    "            print(soup_articles[index][0])\n",
    "    # On retourne la liste de tupes pour la revue concernée\n",
    "    return Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running time < 3min\n",
    "Corpus_AE = create_corpus(soup_articles_AE)\n",
    "Corpus_EI = create_corpus(soup_articles_EI)\n",
    "Corpus_RI = create_corpus(soup_articles_RI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store Corpus_AE\n",
    "%store Corpus_EI\n",
    "%store Corpus_RI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
