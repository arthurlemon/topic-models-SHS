{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline de préparation des données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OBJECTIF GENERAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Etablir un code propre et optimisé de traitement du contenu et métadonnées issus du serveur Erudit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OBJECTIFS SPECIFIQUES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nettoyer le texte \n",
    "- Nettoyer les métadonnées\n",
    "- Filtrer le vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **METHODE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I. Nettoyage du texte**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization : NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enlever mots outils : **NLTK + Spacy stopwords** : STOPWORDS\n",
    "\n",
    "Enlever mots trop courts : longueur <= 2\n",
    "\n",
    "Lemmatizer: **FrenchLeffLemmatizer**\n",
    "\n",
    "Enlever documents \"trop courts\" (< 300 mots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**II. Nettoyage des métadonnées**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Détecter valeurs manquantes pour certains attributs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gros du travail est sur les auteurs : affiliation à nettoyer et Nom/Prénom pour disambiguation mais pas nécessaire ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III. Vocabulaire**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enlever mots rares/fréquents\n",
    "\n",
    "Correcteur orthographique et comparaison avec dictionnaire de référence : **algorithme de Norvig** (http://norvig.com/spell-correct.html) et **dictionnaire FR Gutenberg**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PIPELINE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INPUT** :\n",
    "\n",
    "Corpus = Liste de dictionnaire contenant le texte et métadonnées d'intérêt de chaque article, prêt à être nettoyé.\n",
    "\n",
    "Issu de *Extraction_XML_to_CORPUS.ipynb*\n",
    "\n",
    "**OUTPUT** :\n",
    "\n",
    "- Corpus_clean = Matrices sac de mots pour chaque revue\n",
    "- tokens_bigrams_corpus_clean = liste de tokens pour chaque article de chaque revue\n",
    "- dictionary = dictionnaire filtré pour chaque revue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Bibliothèques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Charge les mots stops en français de NLTK et Spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Ajout des stopwords de NLTK dans ceux de Spacy\n",
    "for word in stopwords.words('french'):\n",
    "    STOP_WORDS.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération d'une liste de dictionnaire  pour chaque revue du type \n",
    "# {'texte' : texte article brut , 'metadata': {'title' : titre, 'typeart': , 'lang': , 'annee': ,'info_auteurs': [(prenom, nom, affiliation)]}, 'URL': }\n",
    "%store -r Corpus_AE\n",
    "%store -r Corpus_EI\n",
    "%store -r Corpus_RI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Nettoyage du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Préprocessing : tokenization et nettoyage avec NLTK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTPUT DE CETTE PARTIE** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% store -r Corpus_LDA_AE\n",
    "% store -r Corpus_LDA_EI\n",
    "% store -r Corpus_LDA_RI\n",
    "\n",
    "%store Corpus_LDA_AE_lemma\n",
    "%store Corpus_LDA_EI_lemma\n",
    "%store Corpus_LDA_RI_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus_LDA (Corpus, stop = STOP_WORDS, lemma = False):\n",
    "    \"\"\" Traitement de chaque article pour tokeniser les mots de chaque article de type article\n",
    "    ATTENTION: les indices Corpus_AE et Corpus_AE_LDA ne sont pas les mêmes car on ne garde que les articles de type article\n",
    "    Se référer à l'URL pour faire la correspondance entre les 2 corpus\"\"\"\n",
    "    \n",
    "    Corpus_LDA = []\n",
    "    \n",
    "    if lemma == True :\n",
    "        lemmatizer = FrenchLefffLemmatizer()\n",
    "    \n",
    "    for index_document in tqdm(range(len(Corpus))) :\n",
    "    # on ne traite que les articles de type article et en français\n",
    "        if (Corpus[index_document]['metadata']['typeart'], Corpus[index_document]['metadata']['lang']) == ('article','fr') : \n",
    "\n",
    "            # tokenization avec NLTK\n",
    "            tokens = word_tokenize(Corpus[index_document]['texte']) \n",
    "\n",
    "            # Enlever les documents trop courts : moins de 300 mots (justifier heuristique)\n",
    "            if len(tokens) < 300 :\n",
    "                pass\n",
    "\n",
    "            # Enlever les chiffres et convertir en minuscules\n",
    "            tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "\n",
    "            # Enlever les mots de moins de 3 caractères (ponctuation) et les stopwords\n",
    "            tokens = [token for token in tokens if len(token) > 2 and token not in stop]       \n",
    "\n",
    "            # Lemmatisation des mots\n",
    "            if lemma == True :\n",
    "                tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "            # Ajout des mots filtrés à la matrice document/mots\n",
    "            Corpus_LDA.append((Corpus[index_document]['URL'], tokens))\n",
    "\n",
    "    return Corpus_LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SANS LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running time = 2 min par revue\n",
    "Corpus_LDA_AE = prepare_corpus_LDA(Corpus_AE, lemma=False)\n",
    "Corpus_LDA_EI = prepare_corpus_LDA(Corpus_EI, lemma = False)\n",
    "Corpus_LDA_RI = prepare_corpus_LDA(Corpus = Corpus_RI, lemma = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store Corpus_LDA_AE\n",
    "%store Corpus_LDA_EI\n",
    "%store Corpus_LDA_RI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AVEC LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running time = 2 min par corpus\n",
    "Corpus_LDA_AE_lemma = prepare_corpus_LDA(Corpus_AE, lemma=True)\n",
    "Corpus_LDA_EI_lemma = prepare_corpus_LDA(Corpus_EI, lemma=True)\n",
    "Corpus_LDA_RI_lemma = prepare_corpus_LDA(Corpus_RI, lemma=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store Corpus_LDA_AE_lemma\n",
    "%store Corpus_LDA_EI_lemma\n",
    "%store Corpus_LDA_RI_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Formation du vocabulaire pour le corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Récupération des bigrammes\n",
    "\n",
    "b) Enlever mots rares/fréquents\n",
    "\n",
    "c) Correcteur orthographique et comparaison avec dictionnaire de référence : **algorithme de Norvig** (https://github.com/barrust/pyspellchecker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USAGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r Corpus_LDA_AE\n",
    "%store -r Corpus_LDA_EI\n",
    "%store -r Corpus_LDA_RI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r Corpus_LDA_AE_lemma\n",
    "%store -r Corpus_LDA_EI_lemma\n",
    "%store -r Corpus_LDA_RI_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  a) Bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des bigrammes\n",
    "def add_bigrams(docs):\n",
    "    from gensim.models.phrases import Phraser, Phrases\n",
    "    phrases = Phrases(docs, min_count=20) # initialisation hyperparamètre = nombre d'occurences d'une paire\n",
    "    bigram = Phraser(phrases) # wrapper plus efficace\n",
    "    for idx in tqdm(range(len(docs))):\n",
    "        for token in bigram[docs[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                docs[idx].append(token)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SANS LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On conserve une liste de tokens pour chaque article en vue de créer le vocabulaire\n",
    "tokens_Corpus_LDA_AE = [element[1] for element in Corpus_LDA_AE]\n",
    "tokens_Corpus_LDA_EI = [element[1] for element in Corpus_LDA_EI]\n",
    "tokens_Corpus_LDA_RI = [element[1] for element in Corpus_LDA_RI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running time : 2min par revue\n",
    "tokens_bigrams_Corpus_LDA_AE= add_bigrams(tokens_Corpus_LDA_AE)\n",
    "tokens_bigrams_Corpus_LDA_EI= add_bigrams(tokens_Corpus_LDA_EI)\n",
    "tokens_bigrams_Corpus_LDA_RI= add_bigrams(tokens_Corpus_LDA_RI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AVEC LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On conserve une liste de tokens pour chaque article en vue de créer le vocabulaire\n",
    "tokens_Corpus_LDA_AE_lemma = [element[1] for element in Corpus_LDA_AE_lemma]\n",
    "tokens_Corpus_LDA_EI_lemma = [element[1] for element in Corpus_LDA_EI_lemma]\n",
    "tokens_Corpus_LDA_RI_lemma = [element[1] for element in Corpus_LDA_RI_lemma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_bigrams_Corpus_LDA_AE_lemma= add_bigrams(tokens_Corpus_LDA_AE_lemma)\n",
    "tokens_bigrams_Corpus_LDA_EI_lemma= add_bigrams(tokens_Corpus_LDA_EI_lemma)\n",
    "tokens_bigrams_Corpus_LDA_RI_lemma= add_bigrams(tokens_Corpus_LDA_RI_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Formation du dictionnaire et nettoyage des mots rares/fréquents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. TRAITEMENT POUR ACTUALITE ECONOMIQUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SANS LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dictionnaire\n",
    "dictionary_AE = Dictionary(tokens_bigrams_Corpus_LDA_AE)\n",
    "print('Nombre de tokens uniques dans les documents après pré-processing:', len(dictionary_AE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AVEC LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dictionnaire\n",
    "dictionary_AE_lemma = Dictionary(tokens_bigrams_Corpus_LDA_AE_lemma)\n",
    "print('Nombre de tokens uniques dans les documents après pré-processing:', len(dictionary_AE_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MOTS RARES ET FREQUENTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Diversité mots**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILTRE ABOVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des dictionnaires selon chaque filtre (à faire une fois pour les 2 stats)\n",
    "filtres_above = list(range(5,105,5))\n",
    "dicos_filtres_above_AE = []\n",
    "for filtre in tqdm(filtres_above):\n",
    "    dico = Dictionary(tokens_bigrams_Corpus_LDA_AE)\n",
    "    dico.filter_extremes(no_above=filtre/100)\n",
    "    dicos_filtres_above_AE.append(dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILTRE BELOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtres_below = list(range(1,20)) # filtre below est en nombre absolu de documents\n",
    "dicos_filtres_below_AE = []\n",
    "for filtre in tqdm(filtres_below):\n",
    "    dico = Dictionary(tokens_bigrams_Corpus_LDA_AE)\n",
    "    dico.filter_extremes(no_below=filtre)\n",
    "    dicos_filtres_below_AE.append(dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLOTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filtre (initiales= 'AE', relatif=True,type_filtre=\"above\"):\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    \n",
    "    if type_filtre == \"above\":\n",
    "        filtres = filtres_above\n",
    "        nb_mots_restants= [len(dico) for dico in dicos_filtres_above_AE]\n",
    "        plt.xlabel('Seuil de filtre des mots fréquents (en pourcentage d\\'articles)',fontsize=20)\n",
    "        if relatif: \n",
    "            nb_mots_restants = [element/nb_mots_restants[-1] for element in nb_mots_restants]\n",
    "            plt.axhline(color = 'r', y=0.98)\n",
    "        else : \n",
    "            plt.axhline(color = 'r', y=30000)\n",
    "        plt.bar(filtres, nb_mots_restants, color='k')\n",
    "    \n",
    "    elif type_filtre == \"below\": \n",
    "        filtres = filtres_below\n",
    "        nb_mots_restants= [len(dico) for dico in dicos_filtres_below_AE]\n",
    "        plt.xlabel('Seuil de filtre des mots rares (en nombre d\\'articles)',fontsize=20)\n",
    "        if relatif: \n",
    "            nb_mots_restants = [element/nb_mots_restants[0] for element in nb_mots_restants]\n",
    "            plt.axhline(color = 'r', y=0.4)\n",
    "        else : \n",
    "            plt.axhline(color = 'r', y=30000)\n",
    "        plt.bar(filtres[0:], nb_mots_restants[0:], color='burlywood')\n",
    "    \n",
    "    plt.ylabel('Taille relative du vocabulaire',fontsize=20)\n",
    "    #plt.title(\"Diversité du vocabulaire en fonction du filtre des mots appaissant dans plus d'un certain pourcent de documents\")\n",
    "    plt.title(initiales, fontsize=40)\n",
    "    plt.savefig(\"Plots/Filtre vocabulaire/filtre_\" + type_filtre+\"_absolu_AE.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filtre (initiales= 'AE', relatif=True,type_filtre=\"below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION**\n",
    "\n",
    "On peut choisir un filtre above à **30% : 90% de diversité des mots est conservée.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On choisit un filtre below à n = 3 documents, ce qui diminue la diversité du vocabulaire de 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) ALGORITHME DE NORVIG ET CORRECTEUR ORTHOGRAPHIQUE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idée : utiliser distance de Levenshtein pour trouver les mots du vocabulaire les plus proches d'un mot donné, et donner la correction la plus probable du mot en fonction de sa fréquence générale dans la langue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker(language='fr', distance=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SANS LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_filtré_AE = [dictionary_AE[w] for w in dictionary_AE]\n",
    "misspelled = spell.unknown(tokens_filtré_AE)\n",
    "corrections = []\n",
    "nb_bigrams = 0\n",
    "for token in misspelled:\n",
    "    if '_' in token: # on ne corrige pas les n_grams mais on les compte\n",
    "         nb_bigrams +=1\n",
    "    else: # on trouve la correction la plus probable pour le token courant\n",
    "        corrections.append((token, spell.correction(token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\", round(len(corrections)/len(tokens_filtré_AE)*100, 1), \"% des mots ont été corrigés.\\n\",\n",
    "      \"Il y a\", nb_bigrams, \"bigrammes dans le dictionnaire filtré ce qui représente \", round(nb_bigrams/len(dictionary_AE)*100,1), \"% des tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_correction = {}\n",
    "for element in corrections:\n",
    "    dico_correction[element[0]]= element[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AVEC LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_filtré_AE_lemma = [dictionary_AE_lemma[w] for w in dictionary_AE_lemma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelled_lemma = spell.unknown(tokens_filtré_AE_lemma)\n",
    "corrections = []\n",
    "nb_bigrams = 0\n",
    "for token in misspelled_lemma:\n",
    "    if '_' in token: # on ne corrige pas les n_grams mais on les compte\n",
    "         nb_bigrams +=1\n",
    "    else: # on trouve la correction la plus probable pour le token courant\n",
    "        corrections.append((token, spell.correction(token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\", round(len(corrections)/len(tokens_filtré_AE_lemma)*100, 1), \"% des mots ont été corrigés.\\n\",\n",
    "      \"Il y a\", nb_bigrams, \"bigrammes dans le dictionnaire filtré ce qui représente \", round(nb_bigrams/len(dictionary_AE_lemma)*100,1), \"% des tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_correction_lemma = {}\n",
    "for element in corrections:\n",
    "    dico_correction_lemma[element[0]]= element[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) FORMATION DU DICTIONNAIRE FINAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMARQUE : il faut appliquer le filtre de correction en 1er en fait, car le dictionnaire gensim ne peut être construit qu'à partir de la représentation liste de liste des documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILTRE CORRECTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SANS LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_bigrams_Corpus_LDA_AE_clean = tokens_bigrams_Corpus_LDA_AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_element in tqdm(range(len(tokens_bigrams_Corpus_LDA_AE))): # parcours de chaque liste de token\n",
    "    for token in tokens_bigrams_Corpus_LDA_AE[index_element]: # évaluation de chaque token \n",
    "        if token in dico_correction: # si le token a été corrigé, le remplacer par sa correction\n",
    "            tokens_bigrams_Corpus_LDA_AE_clean[index_element] = [dico_correction[token] if x == token else x for x in tokens_bigrams_Corpus_LDA_AE_clean[index_element]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store tokens_bigrams_Corpus_LDA_AE_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DICTIONNAIRE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_AE_2= Dictionary(tokens_bigrams_Corpus_LDA_AE_clean)\n",
    "print('Nombre de tokens uniques dans les documents après pré-processing:', len(dictionary_AE_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix des filtres en fonction de la discussion plus haut\n",
    "filtre_above_optim = 0.3\n",
    "filtre_below_optim = 3\n",
    "\n",
    "dictionary_AE_2.filter_extremes(no_below=filtre_below_optim, no_above=filtre_above_optim)\n",
    "print('Nombre de tokens uniques dans les documents après pré-processing:', len(dictionary_AE_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store dictionary_AE_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_bigrams_Corpus_LDA_AE_clean_lemma = tokens_bigrams_Corpus_LDA_AE_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_element in tqdm_notebook(range(len(tokens_bigrams_Corpus_LDA_AE_lemma))): # parcours de chaque liste de token\n",
    "    for token in tokens_bigrams_Corpus_LDA_AE_lemma[index_element]: # évaluation de chaque token \n",
    "        if token in dico_correction: # si le token a été corrigé, le remplacer par sa correction\n",
    "            tokens_bigrams_Corpus_LDA_AE_clean_lemma[index_element] = [dico_correction[token] if x == token else x for x in tokens_bigrams_Corpus_LDA_AE_clean_lemma[index_element]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DICTIONNAIRE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_AE_2_lemma= Dictionary(tokens_bigrams_Corpus_LDA_AE_clean_lemma)\n",
    "print('Nombre de tokens uniques dans les documents après pré-processing:', len(dictionary_AE_2_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix des filtres en fonction de la discussion plus haut\n",
    "filtre_above_optim = 0.3\n",
    "filtre_below_optim = 3\n",
    "\n",
    "dictionary_AE_2_lemma.filter_extremes(no_below=filtre_below_optim, no_above=filtre_above_optim)\n",
    "print('Nombre de tokens uniques dans les documents après pré-processing:', len(dictionary_AE_2_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store tokens_bigrams_Corpus_LDA_AE_clean_lemma\n",
    "%store dictionary_AE_2_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tot(tokens_corpus, dictionary):\n",
    "    \"Retourne le nombre total de tokens dans le corpus\"\n",
    "    count_tot = 0\n",
    "    for document in tqdm_notebook(range(len(tokens_corpus))):\n",
    "        for word in dictionary:\n",
    "            count_tot += tokens_corpus[document].count(dictionary[word])\n",
    "    return count_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sur tous les documents, avec le dictionnaire non filtré\n",
    "# Running time = 1h20\n",
    "count_tot_base_AE_lemma = count_tot(tokens_corpus = tokens_bigrams_Corpus_LDA_AE_clean_lemma, dictionary = dictionary_AE_2_lemma)\n",
    "count_tot_base_AE_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store count_tot_base_AE_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "**RESUME APRES FILTRE SANS LEMMATISATION**\n",
    "- Le vocabulaire contient 36100 tokens uniques (différents)\n",
    "- Le nombre total de tokens est de 4 407 745\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESUME APRES FILTRE APRES LEMMATISATION**\n",
    "- Le vocabulaire contient 32 374 tokens uniques (différents)\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ii TRAITEMENT POUR ETUDES INTERNATIONALES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SANS LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dictionnaire initial\n",
    "dictionary_EI = Dictionary(tokens_bigrams_Corpus_LDA_EI)\n",
    "print('Nombre de tokens uniques dans les documents après pré-processing:', len(dictionary_EI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AVEC LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dictionnaire\n",
    "dictionary_EI_lemma = Dictionary(tokens_bigrams_Corpus_LDA_EI_lemma)\n",
    "print('Nombre de tokens uniques dans les documents après pré-processing:', len(dictionary_EI_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MOTS RARES ET FREQUENTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Diversité mots**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILTRE ABOVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des dictionnaires selon chaque filtre (à faire une fois pour les 2 stats)\n",
    "filtres_above = list(range(5,105,5))\n",
    "dicos_filtres_above_EI = []\n",
    "for filtre in tqdm(filtres_above):\n",
    "    dico = Dictionary(tokens_bigrams_Corpus_LDA_EI)\n",
    "    dico.filter_extremes(no_above=filtre/100)\n",
    "    dicos_filtres_above_EI.append(dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILTRE BELOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtres_below = list(range(1,20)) # filtre below est en nombre absolu de documents\n",
    "dicos_filtres_below_EI = []\n",
    "for filtre in tqdm(filtres_below):\n",
    "    dico = Dictionary(tokens_bigrams_Corpus_LDA_EI)\n",
    "    dico.filter_extremes(no_below=filtre)\n",
    "    dicos_filtres_below_EI.append(dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLOTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filtre (initiales, relatif=True,type_filtre=\"above\"):\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    \n",
    "    if type_filtre == \"above\":\n",
    "        filtres = filtres_above\n",
    "        nb_mots_restants= [len(dico) for dico in dicos_filtres_above_EI]\n",
    "        plt.xlabel('Seuil de filtre des mots fréquents (en pourcentage d\\'articles)',fontsize=20)\n",
    "        if relatif: \n",
    "            nb_mots_restants = [element/nb_mots_restants[-1] for element in nb_mots_restants]\n",
    "            plt.axhline(color = 'r', y=0.97)\n",
    "        else : \n",
    "            plt.axhline(color = 'r', y=30000)\n",
    "        plt.bar(filtres, nb_mots_restants, color='k')\n",
    "    \n",
    "    elif type_filtre == \"below\": \n",
    "        filtres = filtres_below\n",
    "        nb_mots_restants= [len(dico) for dico in dicos_filtres_below_EI]\n",
    "        plt.xlabel('Seuil de filtre des mots rares (en nombre d\\'articles)',fontsize=20)\n",
    "        if relatif: \n",
    "            nb_mots_restants = [element/nb_mots_restants[0] for element in nb_mots_restants]\n",
    "            plt.axhline(color = 'r', y=0.46)\n",
    "        else : \n",
    "            plt.axhline(color = 'r', y=30000)\n",
    "        plt.bar(filtres, nb_mots_restants, color='burlywood')\n",
    "    \n",
    "    plt.ylabel('Taille relative du vocabulaire',fontsize=20)\n",
    "    #plt.title(\"Diversité du vocabulaire en fonction du filtre des mots appaissant dans plus d'un certain pourcent de documents\")\n",
    "    plt.title(initiales, fontsize=40)\n",
    "    plt.savefig(\"Plots/Filtre vocabulaire/filtre_\" + type_filtre+\"_absolu_\"+initiales+\".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filtre (initiales= 'EI', relatif=True,type_filtre=\"below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) ALGORITHME DE NORVIG ET CORRECTEUR ORTHOGRAPHIQUE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idée : utiliser distance de Levenshtein pour trouver les mots du vocabulaire les plus proches d'un mot donné, et donner la correction la plus probable du mot en fonction de sa fréquence générale dans la langue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker(language='fr', distance=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SANS LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_filtré_EI = [dictionary_EI[w] for w in dictionary_EI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelled = spell.unknown(tokens_filtré_EI)\n",
    "corrections = []\n",
    "nb_bigrams = 0\n",
    "for token in misspelled:\n",
    "    if '_' in token: # on ne corrige pas les n_grams mais on les compte\n",
    "         nb_bigrams +=1\n",
    "    else: # on trouve la correction la plus probable pour le token courant\n",
    "        corrections.append((token, spell.correction(token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\", round(len(corrections)/len(tokens_filtré_EI)*100, 1), \"% des mots ont été corrigés.\\n\",\n",
    "      \"Il y a\", nb_bigrams, \"bigrammes dans le dictionnaire filtré ce qui représente \", round(nb_bigrams/len(dictionary_EI)*100,1), \"% des tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_correction = {}\n",
    "for element in corrections:\n",
    "    dico_correction[element[0]]= element[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AVEC LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_filtré_EI_lemma = [dictionary_EI_lemma[w] for w in dictionary_EI_lemma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelled_lemma = spell.unknown(tokens_filtré_EI_lemma)\n",
    "corrections = []\n",
    "nb_bigrams = 0\n",
    "for token in misspelled_lemma:\n",
    "    if '_' in token: # on ne c_lemmaorrige pas les n_grams mais on les compte\n",
    "         nb_bigrams +=1\n",
    "    else: # on trouve la correction la plus probable pour le token courant\n",
    "        corrections.append((token, spell.correction(token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\", round(len(corrections)/len(tokens_filtré_EI_lemma)*100, 1), \"% des mots ont été corrigés.\\n\",\n",
    "      \"Il y a\", nb_bigrams, \"bigrammes dans le dictionnaire filtré ce qui représente \", round(nb_bigrams/len(dictionary_EI_lemma)*100,1), \"% des tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_correction = {}\n",
    "for element in corrections:\n",
    "    dico_correction[element[0]]= element[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) FORMATION DU DICTIONNAIRE FINAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SANS LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_bigrams_Corpus_LDA_EI_clean = tokens_bigrams_Corpus_LDA_EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_element in tqdm_notebook(range(len(tokens_bigrams_Corpus_LDA_EI))): # parcours de chaque liste de token\n",
    "    for token in tokens_bigrams_Corpus_LDA_EI[index_element]: # évaluation de chaque token \n",
    "        if token in dico_correction: # si le token a été corrigé, le remplacer par sa correction\n",
    "            tokens_bigrams_Corpus_LDA_EI_clean[index_element] = [dico_correction[token] if x == token else x for x in tokens_bigrams_Corpus_LDA_EI_clean[index_element]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store tokens_bigrams_Corpus_LDA_AE_clean_lemma\n",
    "%store dictionary_AE_2_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AVEC LEMMATISATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i.FILTRE CORRECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_bigrams_Corpus_LDA_EI_clean_lemma = tokens_bigrams_Corpus_LDA_EI_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_element in tqdm_notebook(range(len(tokens_bigrams_Corpus_LDA_EI_lemma))): # parcours de chaque liste de token\n",
    "    for token in tokens_bigrams_Corpus_LDA_EI_lemma[index_element]: # évaluation de chaque token \n",
    "        if token in dico_correction: # si le token a été corrigé, le remplacer par sa correction\n",
    "            tokens_bigrams_Corpus_LDA_EI_clean_lemma[index_element] = [dico_correction[token] if x == token else x for x in tokens_bigrams_Corpus_LDA_EI_clean_lemma[index_element]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii. DICTIONNAIRE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_EI_2_lemma= Dictionary(tokens_bigrams_Corpus_LDA_EI_clean_lemma)\n",
    "print('Nombre de tokens uniques dans les documents après pré-processing:', len(dictionary_EI_2_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix des filtres en fonction de la discussion plus haut\n",
    "filtre_above_optim = 0.3\n",
    "filtre_below_optim = 3\n",
    "\n",
    "dictionary_EI_2_lemma.filter_extremes(no_below=filtre_below_optim, no_above=filtre_above_optim)\n",
    "print('Nombre de tokens uniques dans les documents après pré-processing:', len(dictionary_EI_2_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store dictionary_EI_2_lemma\n",
    "%store tokens_bigrams_Corpus_LDA_EI_clean_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Nombre total de tokens dans le corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r tokens_bigrams_Corpus_LDA_EI_clean\n",
    "%store -r dictionary_EI_2\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tot(tokens_corpus, dictionary):\n",
    "    \"Retourne le nombre total de tokens dans le corpus\"\n",
    "    count_tot = 0\n",
    "    for document in tqdm_notebook(range(len(tokens_corpus))):\n",
    "        for word in dictionary:\n",
    "            count_tot += tokens_corpus[document].count(dictionary[word])\n",
    "    return count_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sur tous les documents, avec le dictionnaire filtré\n",
    "# Running time = 1h\n",
    "count_tot_base_EI = count_tot(tokens_corpus = tokens_bigrams_Corpus_LDA_EI_clean, dictionary = dictionary_EI_2)\n",
    "count_tot_base_EI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "**RESUME APRES FILTRE SANS LEMMATISATION**\n",
    "- Le vocabulaire contient 41.585 tokens uniques\n",
    "- Le nombre total de tokens est de 2 306 993\n",
    "- Le nombre total d'articles est de 1094\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESUME APRES FILTRE AVEC LEMMATISATION**\n",
    "- Le vocabulaire contient 37.343 tokens uniques\n",
    "- Le nombre total de tokens est de \n",
    "- Le nombre total d'articles est de 1094\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store count_tot_base_EI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **iii. TRAITEMENT POUR RELATIONS INDUSTRIELLES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SANS LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dictionnaire initial\n",
    "dictionary_RI = Dictionary(tokens_bigrams_Corpus_LDA_RI)\n",
    "print('Nombre de tokens uniques dans les documents avant pré-processing:', len(dictionary_RI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AVEC LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dictionnaire\n",
    "dictionary_RI_lemma = Dictionary(tokens_bigrams_Corpus_LDA_RI_lemma)\n",
    "print('Nombre de tokens uniques dans les documents après pré-processing:', len(dictionary_RI_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MOTS RARES ET FREQUENTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Diversité mots**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILTRE ABOVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des dictionnaires selon chaque filtre (à faire une fois pour les 2 stats)\n",
    "filtres_above = list(range(5,105,5))\n",
    "dicos_filtres_above_RI = []\n",
    "for filtre in tqdm(filtres_above):\n",
    "    dico = Dictionary(tokens_bigrams_Corpus_LDA_RI)\n",
    "    dico.filter_extremes(no_above=filtre/100)\n",
    "    dicos_filtres_above_RI.append(dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILTRE BELOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtres_below = list(range(1,20)) # filtre below est en nombre absolu de documents\n",
    "dicos_filtres_below_RI = []\n",
    "for filtre in tqdm(filtres_below):\n",
    "    dico = Dictionary(tokens_bigrams_Corpus_LDA_RI)\n",
    "    dico.filter_extremes(no_below=filtre)\n",
    "    dicos_filtres_below_RI.append(dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLOTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filtre (initiales, relatif=True,type_filtre=\"above\"):\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    \n",
    "    if type_filtre == \"above\":\n",
    "        filtres = filtres_above\n",
    "        nb_mots_restants= [len(dico) for dico in dicos_filtres_above_RI]\n",
    "        plt.xlabel('Seuil de filtre des mots fréquents (en pourcentage d\\'articles)',fontsize=20)\n",
    "        if relatif: \n",
    "            nb_mots_restants = [element/nb_mots_restants[-1] for element in nb_mots_restants]\n",
    "            plt.axhline(color = 'r', y=0.98)\n",
    "        else : \n",
    "            plt.axhline(color = 'r', y=30000)\n",
    "        plt.bar(filtres, nb_mots_restants, color='k')\n",
    "    \n",
    "    elif type_filtre == \"below\": \n",
    "        filtres = filtres_below\n",
    "        nb_mots_restants= [len(dico) for dico in dicos_filtres_below_RI]\n",
    "        plt.xlabel('Seuil de filtre des mots rares (en nombre d\\'articles)',fontsize=20)\n",
    "        if relatif: \n",
    "            nb_mots_restants = [element/nb_mots_restants[0] for element in nb_mots_restants]\n",
    "            plt.axhline(color = 'r', y=0.45)\n",
    "        else : \n",
    "            plt.axhline(color = 'r', y=30000)\n",
    "        plt.bar(filtres, nb_mots_restants, color='burlywood')\n",
    "    \n",
    "    plt.ylabel('Taille relative du vocabulaire',fontsize=20)\n",
    "    #plt.title(\"Diversité du vocabulaire en fonction du filtre des mots appaissant dans plus d'un certain pourcent de documents\")\n",
    "    plt.title(initiales, fontsize=40)\n",
    "    plt.savefig(\"Plots/Filtre vocabulaire/filtre_\" + type_filtre+\"_absolu_\"+initiales+\".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filtre (initiales= 'RI', relatif=True,type_filtre=\"below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filtre (initiales= 'RI', relatif=True,type_filtre=\"below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) ALGORITHME DE NORVIG ET CORRECTEUR ORTHOGRAPHIQUE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idée : utiliser distance de Levenshtein pour trouver les mots du vocabulaire les plus proches d'un mot donné, et donner la correction la plus probable du mot en fonction de sa fréquence générale dans la langue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker(language='fr', distance=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SANS LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_filtré_RI = [dictionary_RI[w] for w in dictionary_RI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelled = spell.unknown(tokens_filtré_RI)\n",
    "corrections = []\n",
    "nb_bigrams = 0\n",
    "for token in misspelled:\n",
    "    if '_' in token: # on ne corrige pas les n_grams mais on les compte\n",
    "         nb_bigrams +=1\n",
    "    else: # on trouve la correction la plus probable pour le token courant\n",
    "        corrections.append((token, spell.correction(token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\", round(len(corrections)/len(tokens_filtré_RI)*100, 1), \"% des mots ont été corrigés.\\n\",\n",
    "      \"Il y a\", nb_bigrams, \"bigrammes dans le dictionnaire filtré ce qui représente \", round(nb_bigrams/len(dictionary_RI)*100,1), \"% des tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_correction = {}\n",
    "for element in corrections:\n",
    "    dico_correction[element[0]]= element[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AVEC LEMMATISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_filtré_RI_lemma = [dictionary_RI_lemma[w] for w in dictionary_RI_lemma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelled_lemma = spell.unknown(tokens_filtré_RI_lemma)\n",
    "corrections = []\n",
    "nb_bigrams = 0\n",
    "for token in misspelled_lemma:\n",
    "    if '_' in token: # on ne corrige pas les n_grams mais on les compte\n",
    "         nb_bigrams +=1\n",
    "    else: # on trouve la correction la plus probable pour le token courant\n",
    "        corrections.append((token, spell.correction(token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\", round(len(corrections)/len(tokens_filtré_RI_lemma)*100, 1), \"% des mots ont été corrigés.\\n\",\n",
    "      \"Il y a\", nb_bigrams, \"bigrammes dans le dictionnaire filtré ce qui représente \", round(nb_bigrams/len(dictionary_RI_lemma)*100,1), \"% des tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_correction = {}\n",
    "for element in corrections:\n",
    "    dico_correction[element[0]]= element[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) FORMATION DU DICTIONNAIRE FINAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SANS LEMMATISATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i. FILTRE CORRECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_bigrams_Corpus_LDA_RI_clean = tokens_bigrams_Corpus_LDA_RI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_element in tqdm_notebook(range(len(tokens_bigrams_Corpus_LDA_RI))): # parcours de chaque liste de token\n",
    "    for token in tokens_bigrams_Corpus_LDA_RI[index_element]: # évaluation de chaque token \n",
    "        if token in dico_correction: # si le token a été corrigé, le remplacer par sa correction\n",
    "            tokens_bigrams_Corpus_LDA_RI_clean[index_element] = [dico_correction[token] if x == token else x for x in tokens_bigrams_Corpus_LDA_RI_clean[index_element]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii.DICTIONNAIRE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_RI_2= Dictionary(tokens_bigrams_Corpus_LDA_RI_clean)\n",
    "print('Nombre de tokens uniques dans les documents après pré-processing:', len(dictionary_RI_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix des filtres en fonction de la discussion plus haut\n",
    "filtre_above_optim = 0.3\n",
    "filtre_below_optim = 3\n",
    "\n",
    "dictionary_RI_2.filter_extremes(no_below=filtre_below_optim, no_above=filtre_above_optim)\n",
    "print('Nombre de tokens uniques dans les documents après pré-processing:', len(dictionary_RI_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store dictionary_RI_2\n",
    "%store tokens_bigrams_Corpus_LDA_RI_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AVEC LEMMATISATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i.FILTRE CORRECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_bigrams_Corpus_LDA_RI_clean_lemma = tokens_bigrams_Corpus_LDA_RI_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_element in tqdm_notebook(range(len(tokens_bigrams_Corpus_LDA_RI_lemma))): # parcours de chaque liste de token\n",
    "    for token in tokens_bigrams_Corpus_LDA_RI_lemma[index_element]: # évaluation de chaque token \n",
    "        if token in dico_correction: # si le token a été corrigé, le remplacer par sa correction\n",
    "            tokens_bigrams_Corpus_LDA_RI_clean_lemma[index_element] = [dico_correction[token] if x == token else x for x in tokens_bigrams_Corpus_LDA_RI_clean_lemma[index_element]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii. DICTIONNAIRE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_RI_2_lemma= Dictionary(tokens_bigrams_Corpus_LDA_RI_clean_lemma)\n",
    "print('Nombre de tokens uniques dans les documents avant pré-processing:', len(dictionary_RI_2_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix des filtres en fonction de la discussion plus haut\n",
    "filtre_above_optim = 0.3\n",
    "filtre_below_optim = 3\n",
    "\n",
    "dictionary_RI_2_lemma.filter_extremes(no_below=filtre_below_optim, no_above=filtre_above_optim)\n",
    "print('Nombre de tokens uniques dans les documents après pré-processing:', len(dictionary_RI_2_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store dictionary_RI_2_lemma\n",
    "%store tokens_bigrams_Corpus_LDA_RI_clean_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Nombre total de tokens dans le corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r tokens_bigrams_Corpus_LDA_RI_clean_lemma\n",
    "%store -r dictionary_RI_2_lemma\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tot(tokens_corpus, dictionary):\n",
    "    \"Retourne le nombre total de tokens dans le corpus\"\n",
    "    count_tot = 0\n",
    "    for document in tqdm_notebook(range(len(tokens_corpus))):\n",
    "        for word in dictionary:\n",
    "            count_tot += tokens_corpus[document].count(dictionary[word])\n",
    "    return count_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sur tous les documents, avec le dictionnaire filtré\n",
    "# Running time = 1h\n",
    "count_tot_base_RI = count_tot(tokens_corpus = tokens_bigrams_Corpus_LDA_RI_clean, dictionary = dictionary_RI_2)\n",
    "count_tot_base_RI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store count_tot_base_RI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "**RESUME APRES FILTRE SANS LEMMATISATION**\n",
    "- Le vocabulaire contient 39205\n",
    "- Le nombre total de tokens est de 2486175\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "**RESUME APRES FILTRE APRES LEMMATISATION**\n",
    "- Le vocabulaire contient 97 333\n",
    "- Le nombre total de tokens est de \n",
    "\n",
    "-----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
